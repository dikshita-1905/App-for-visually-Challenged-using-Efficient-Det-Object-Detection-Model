{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " TfliteObjectDetectionNewVersion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8H-wevUgEDs",
        "outputId": "0210a5b0-49b8-4596-e635-7d41154cf244"
      },
      "source": [
        "!pip install -q tflite-model-maker\n",
        "!pip install -q pycocotools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 591 kB 13.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 49.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 53.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 679 kB 26.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 840 kB 51.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 103 kB 54.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 211 kB 51.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 47.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 35.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 39.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.1 MB 53 kB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 52.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 200 kB 50.3 MB/s \n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaG0pIhOgGyH"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY9PAXyegMqF"
      },
      "source": [
        "spec = model_spec.get('efficientdet_lite2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, validation_data, test_data = object_detector.DataLoader.from_csv('gs://cloud-ml-data/img/openimage/csv/salads_ml_use.csv')"
      ],
      "metadata": {
        "id": "gI4JWU3tjAAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drNJ0r_Nl5wv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c97756-ee8b-4f82-fa53-1249c8f2eb6f"
      },
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, epochs=80, batch_size=8, train_whole_model=True, validation_data=validation_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "12/12 [==============================] - 64s 2s/step - det_loss: 1.7408 - cls_loss: 1.1298 - box_loss: 0.0122 - reg_l2_loss: 0.0759 - loss: 1.8167 - learning_rate: 0.0090 - gradient_norm: 1.9155 - val_det_loss: 1.6286 - val_cls_loss: 1.0500 - val_box_loss: 0.0116 - val_reg_l2_loss: 0.0759 - val_loss: 1.7045\n",
            "Epoch 2/80\n",
            " 7/12 [================>.............] - ETA: 5s - det_loss: 1.5358 - cls_loss: 1.0010 - box_loss: 0.0107 - reg_l2_loss: 0.0759 - loss: 1.6116 - learning_rate: 0.0100 - gradient_norm: 3.1561"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjQvUfcimCtz"
      },
      "source": [
        "model.evaluate(validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYxXfN5NmHKF"
      },
      "source": [
        "model.export(export_dir='.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua6jo4DmmLth"
      },
      "source": [
        "model.evaluate_tflite('model.tflite', validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zq_rfcrmPSg"
      },
      "source": [
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "model_path = 'model.tflite'\n",
        "\n",
        "# Load the labels into a list\n",
        "classes = ['???'] * model.model_spec.config.num_classes\n",
        "label_map = model.model_spec.config.label_map\n",
        "for label_id, label_name in label_map.as_dict().items():\n",
        "  classes[label_id-1] = label_name\n",
        "\n",
        "# Define a list of colors for visualization\n",
        "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
        "\n",
        "def preprocess_image(image_path, input_size):\n",
        "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
        "  img = tf.io.read_file(image_path)\n",
        "  img = tf.io.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
        "  original_image = img\n",
        "  resized_img = tf.image.resize(img, input_size)\n",
        "  resized_img = resized_img[tf.newaxis, :]\n",
        "  return resized_img, original_image\n",
        "\n",
        "\n",
        "def set_input_tensor(interpreter, image):\n",
        "  \"\"\"Set the input tensor.\"\"\"\n",
        "  tensor_index = interpreter.get_input_details()[0]['index']\n",
        "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
        "  input_tensor[:, :] = image\n",
        "\n",
        "\n",
        "def get_output_tensor(interpreter, index):\n",
        "  \"\"\"Retur the output tensor at the given index.\"\"\"\n",
        "  output_details = interpreter.get_output_details()[index]\n",
        "  tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
        "  return tensor\n",
        "\n",
        "\n",
        "def detect_objects(interpreter, image, threshold):\n",
        "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
        "  # Feed the input image to the model\n",
        "  set_input_tensor(interpreter, image)\n",
        "  interpreter.invoke()\n",
        "\n",
        "  # Get all outputs from the model\n",
        "  boxes = get_output_tensor(interpreter, 0)\n",
        "  classes = get_output_tensor(interpreter, 1)\n",
        "  scores = get_output_tensor(interpreter, 2)\n",
        "  count = int(get_output_tensor(interpreter, 3))\n",
        "\n",
        "  results = []\n",
        "  for i in range(count):\n",
        "    if scores[i] >= threshold:\n",
        "      result = {\n",
        "        'bounding_box': boxes[i],\n",
        "        'class_id': classes[i],\n",
        "        'score': scores[i]\n",
        "      }\n",
        "      results.append(result)\n",
        "  return results\n",
        "\n",
        "\n",
        "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
        "  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
        "  # Load the input shape required by the model\n",
        "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
        "\n",
        "  # Load the input image and preprocess it\n",
        "  preprocessed_image, original_image = preprocess_image(\n",
        "      image_path, \n",
        "      (input_height, input_width)\n",
        "    )\n",
        "\n",
        "  # Run object detection on the input image\n",
        "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
        "\n",
        "  # Plot the detection results on the input image\n",
        "  original_image_np = original_image.numpy().astype(np.uint8)\n",
        "  for obj in results:\n",
        "    # Convert the object bounding box from relative coordinates to absolute \n",
        "    # coordinates based on the original image resolution\n",
        "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
        "    xmin = int(xmin * original_image_np.shape[1])\n",
        "    xmax = int(xmax * original_image_np.shape[1])\n",
        "    ymin = int(ymin * original_image_np.shape[0])\n",
        "    ymax = int(ymax * original_image_np.shape[0])\n",
        "\n",
        "    # Find the class index of the current object\n",
        "    class_id = int(obj['class_id'])\n",
        "\n",
        "    # Draw the bounding box and label on the image\n",
        "    color = [int(c) for c in COLORS[class_id]]\n",
        "    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
        "    # Make adjustments to make the label visible for all objects\n",
        "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
        "    label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
        "    cv2.putText(original_image_np, label, (xmin, y),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "  # Return the final image\n",
        "  original_uint8 = original_image_np.astype(np.uint8)\n",
        "  return original_uint8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7Yf6KItmYIb"
      },
      "source": [
        "INPUT_IMAGE_URL = \"/content/microcontroller-detection/test/IMG_20181228_102641.jpg\"\n",
        "DETECTION_THRESHOLD = 0.5 \n",
        "\n",
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Run inference and draw detection result on the local copy of the original file\n",
        "detection_result_image = run_odt_and_draw_results(\n",
        "    INPUT_IMAGE_URL, \n",
        "    interpreter, \n",
        "    threshold=DETECTION_THRESHOLD\n",
        ")\n",
        "\n",
        "# Show the detection result\n",
        "Image.fromarray(detection_result_image)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}